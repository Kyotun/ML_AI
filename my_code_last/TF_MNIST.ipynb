{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00bfea91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cceec452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08180e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "#name='path of file'\n",
    "#with_info= provides a tuple containing info about version, features and # samples of the data set\n",
    "#as_supervised = True, loads the data in a 2-tuple structure [input,target]\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "#extracting data\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "#validation comes from train part, there is no specific part for validation\n",
    "#train contains 60,000 examples\n",
    "#test contains 10,000 examples\n",
    "\n",
    "#define the number of validation samples\n",
    "#but we don't know if this num is integer or float\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "#therefore, we convert the variable into a given data type\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "#store the num of test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "#normally we'd like to scale the datain some way to make the result more numerically stable (inputs between 0-1)\n",
    "\n",
    "#take an MNIST iamge and its label\n",
    "#0-255\n",
    "#we want float\n",
    "#you can scale your data in other ways if you see fit.\n",
    "#make sure that func. takes image and label\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "#scale the whole train dataset and store it in variable\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "#what if every batch consist different procent of same element?\n",
    "#that is why, we need to shuffle the data as random as possible, in order to get the best results\n",
    "#therefore, we need buffer in order to put all data and shuffle in that buffer the all data at one time\n",
    "#if buffer size >= num_sample, shuffling will happen at once (uniformly)\n",
    "#if 1 < buffer size <= num_sample, we'll be optimizing the computational power of computer\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "#after that we can extract the exact train and validation sets\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "#.skip() method skips the given parameter and take the datas what is left from (all-given parameter)\n",
    "#we'll use minibatch GD -> Sample size / batch size = element_in_one_batch\n",
    "#batch_size = 1 -> SGD\n",
    "#batch_size = # samples GD(single batch)\n",
    "#1<batch_size z # samples -> mini batch\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "#.batch(SIZE) -> put the given data into the given sizes of batches\n",
    "train_data = train_data.batch(BATCH_SIZE)#update the weights every batch(in every 100 element)\n",
    "#we need to reshape validation_data into batch format too but when we forward propagate we need just 1 batch cause\n",
    "#we take all the datas at once, because we want exact values(not updating in every 100 element)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "#validation data must have the same shape and obj. properties as the train and test data\n",
    "#remember that we did as_supervised -> loads the data in 2-Tuple structure [inputs,targets]\n",
    "#therefore, we need extract and convert the validation inputs/targets too\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "#.iter() = creates an obj. which can be iterated one element at a time\n",
    "#.next() = loads the next element of an iterable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6bdfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 784 input, 10 output nodes, 2 hidden layer 50 nodes each\n",
    "#you can create different width hidden layers if it works better\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2ad04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#model.compile(optimizer=custom_optimizer, loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#loss func for classifiers\n",
    "#there are 3 types of cross entropy -> binary, categorical, sparse_categorical\n",
    "#binary -> case in that we got binary encoding\n",
    "#categorical -> expects that you've one-hot encoded the targets\n",
    "#sparse_categorical -> applies one-hot encoding\n",
    "#increasing the learning rate from 0.0001 to 0.001 gives us better acc, can take a little bit more time or equal\n",
    "#using 'adam' as optimizer gives us bitter results rather than custom_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e638ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.7637 - accuracy: 0.8164 - val_loss: 0.3088 - val_accuracy: 0.9162 - 5s/epoch - 9ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 5s - loss: 0.2564 - accuracy: 0.9266 - val_loss: 0.2265 - val_accuracy: 0.9377 - 5s/epoch - 9ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 5s - loss: 0.1949 - accuracy: 0.9429 - val_loss: 0.1847 - val_accuracy: 0.9495 - 5s/epoch - 9ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 5s - loss: 0.1577 - accuracy: 0.9537 - val_loss: 0.1564 - val_accuracy: 0.9583 - 5s/epoch - 9ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 5s - loss: 0.1314 - accuracy: 0.9614 - val_loss: 0.1307 - val_accuracy: 0.9642 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2152cac8c70>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training part\n",
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "#if you decrease the batch_size acc increases but the time increases too\n",
    "#if you increase the hidden_layer_size, acc increases too but the time increases itself too\n",
    "#adding more layers can lower the acc and increase the time, not effektiv\n",
    "\n",
    "#these accs are validation accs. So it is possible that we overfitted the data\n",
    "#we have to test it too with forward propagating the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bde96e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 661ms/step - loss: 0.1317 - accuracy: 0.9604\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "test_loss, test_acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ede3515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.13. Test accuracy:  96.04%\n"
     ]
    }
   ],
   "source": [
    "print(('Test loss: {0:.2f}. Test accuracy: {1: .2f}%').format(test_loss, test_acc*100.))\n",
    "#we can print it too.\n",
    "#Test accuracy shows that we did not overfit afterall.\n",
    "#If the model would change, test set should be evaluated from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6432dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by the testing the model, relu > tanh > sigmoid, for this example\n",
    "#time parameter -> relu 24s, tanh 26s, sigmoid 25s\n",
    "#accuracy parameter -> relu 97.38, tanh 97.23, sigmoid 96.04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
